{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aee751f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple, List, Any\n",
    "import datetime as dt\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "import mlflow\n",
    "from category_encoders import CountEncoder, WOEEncoder\n",
    "from kedro.framework.session import KedroSession\n",
    "from kedro.framework.startup import bootstrap_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "010b9176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-27 03:08:16,356 - kedro.framework.session.store - INFO - `save()` not implemented for `BaseSessionStore`. Skipping the step.\n"
     ]
    }
   ],
   "source": [
    "# use config with base\n",
    "metadata = bootstrap_project(Path.cwd().parent.parent)\n",
    "with KedroSession.create(metadata.package_name,\n",
    "        project_path=metadata.project_path,\n",
    "        # save_on_close=True,\n",
    "        env=None,\n",
    "        # extra_params=extra_params\n",
    "    ) as session: \n",
    "    context = session.load_context()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23f6634e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_size': 0.2,\n",
       " 'random_state': 3,\n",
       " 'features': ['engines',\n",
       "  'passenger_capacity',\n",
       "  'crew',\n",
       "  'company_rating',\n",
       "  'review_scores_rating',\n",
       "  'd_check_complete',\n",
       "  'moon_clearance_complete',\n",
       "  'iata_approved'],\n",
       " 'categorical_features': ['d_check_complete',\n",
       "  'moon_clearance_complete',\n",
       "  'iata_approved'],\n",
       " 'model_params': {'hyper_parameters': {'max_depth': 7,\n",
       "   'n_estimators': 30,\n",
       "   'random_state': 555}}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da946bc1",
   "metadata": {
    "tags": [
     "node"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "# regressor = xgb.sklearn.XGBRegressor(**context.params[\"model_params\"][\"hyper_parameters\"])\n",
    "regressor = xgb.sklearn.XGBClassifier(**context.params[\"model_params\"][\"hyper_parameters\"])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"ce\", WOEEncoder()),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"float_input\", \"passthrough\", [context.params[\"features\"].index(cf) for cf in context.params[\"features\"] if cf not in context.params[\"categorical_features\"]]),\n",
    "        (\"categorical_input\", categorical_transformer, [context.params[\"features\"].index(cf) for cf in context.params[\"categorical_features\"]]),\n",
    "    ],\n",
    "    remainder=\"drop\")\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    # Bad!\n",
    "    # (\"selector\", ColumnTransformer(\n",
    "    #     [(\"id\", \n",
    "    #       FunctionTransformer(validate=False),\n",
    "    #       list(range(len(context.params[\"features\"]))))\n",
    "    #     ])\n",
    "    # ),\n",
    "    # (\"selector\", CoulmnTransformer(FunctionTransformer(lambda X: X[:, list(range(len(context.params[\"features\"])))], validate=False))),\n",
    "    (\"precprocessor\", preprocessor),\n",
    "    (\"regressor\", regressor)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4541b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[context.params[\"features\"].index(cf) for cf in context.params[\"features\"] if cf not in context.params[\"categorical_features\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "113238f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 7]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[context.params[\"features\"].index(cf) for cf in context.params[\"categorical_features\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16d0eb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/kedro/framework/context/context.py:488: UserWarning: Credentials not found in your Kedro project config.\n",
      "No files found in ['/kedro-sample/own_examples/conf/base', '/kedro-sample/own_examples/conf/local'] matching the glob pattern(s): ['credentials*', 'credentials*/**', '**/credentials*']\n",
      "  warn(f\"Credentials not found in your Kedro project config.\\n{str(exc)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-27 03:08:17,325 - kedro.io.data_catalog - INFO - Loading data from `X_train` (PickleDataSet)...\n"
     ]
    }
   ],
   "source": [
    "tmp = context.catalog.load(\"X_train\")\n",
    "tmp[context.params[\"categorical_features\"]] = tmp[context.params[\"categorical_features\"]].astype(str)\n",
    "tmp[\"aaaaa\"] = tmp.engines.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46392b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-27 03:08:18,090 - kedro.io.data_catalog - INFO - Loading data from `y_train` (PickleDataSet)...\n",
      "2021-12-27 03:08:18,122 - kedro.io.data_catalog - INFO - Loading data from `y_train` (PickleDataSet)...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(context.catalog.load(\"y_train\")<context.catalog.load(\"y_train\").mean(), 0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25cde79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-27 03:08:18,464 - kedro.io.data_catalog - INFO - Loading data from `y_train` (PickleDataSet)...\n",
      "2021-12-27 03:08:18,486 - kedro.io.data_catalog - INFO - Loading data from `y_train` (PickleDataSet)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:08:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "ppl = model.fit(\n",
    "    # tmp[context.params[\"features\"]].values\n",
    "    tmp[context.params[\"features\"]+[\"aaaaa\"]].values\n",
    "    # ,context.catalog.load(\"y_train\")\n",
    "    ,np.where(context.catalog.load(\"y_train\")<context.catalog.load(\"y_train\").mean(), 0,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e382d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7748882 , 0.22511178],\n",
       "       [0.00519294, 0.99480706],\n",
       "       [0.9798459 , 0.0201541 ],\n",
       "       ...,\n",
       "       [0.9973383 , 0.00266168],\n",
       "       [0.9860327 , 0.01396728],\n",
       "       [0.80912876, 0.19087121]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppl.predict_proba(tmp[context.params[\"features\"]+[\"aaaaa\"]].values)\n",
    "# ppl.predict(tmp[context.params[\"features\"]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48448c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-27 03:08:23,125 - kedro.io.data_catalog - INFO - Loading data from `y_train` (PickleDataSet)...\n",
      "2021-12-27 03:08:23,146 - kedro.io.data_catalog - INFO - Loading data from `y_train` (PickleDataSet)...\n",
      "[03:08:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "ppl = model.fit(\n",
    "    tmp[context.params[\"features\"]].values\n",
    "    # ,context.catalog.load(\"y_train\")\n",
    "    ,np.where(context.catalog.load(\"y_train\")<context.catalog.load(\"y_train\").mean(), 0,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98597110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppl.predict(tmp[context.params[\"features\"]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb123c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>engines</th>\n",
       "      <th>passenger_capacity</th>\n",
       "      <th>crew</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>review_scores_rating</th>\n",
       "      <th>d_check_complete</th>\n",
       "      <th>moon_clearance_complete</th>\n",
       "      <th>iata_approved</th>\n",
       "      <th>aaaaa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115794</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238624</th>\n",
       "      <td>4.0</td>\n",
       "      <td>8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389153</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628283</th>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452204</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        engines  passenger_capacity  crew  company_rating  \\\n",
       "115794      2.0                   4   3.0             1.0   \n",
       "238624      4.0                   8   5.0             1.0   \n",
       "389153      1.0                   2   1.0             1.0   \n",
       "628283      2.0                   6   2.0             1.0   \n",
       "452204      1.0                   2   1.0             1.0   \n",
       "\n",
       "        review_scores_rating d_check_complete moon_clearance_complete  \\\n",
       "115794                  96.0            False                   False   \n",
       "238624                 100.0             True                   False   \n",
       "389153                  65.0             True                   False   \n",
       "628283                 100.0             True                   False   \n",
       "452204                  60.0             True                   False   \n",
       "\n",
       "       iata_approved  aaaaa  \n",
       "115794          True    2.0  \n",
       "238624         False    4.0  \n",
       "389153         False    1.0  \n",
       "628283         False    2.0  \n",
       "452204         False    1.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[context.params[\"features\"]+[\"aaaaa\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdf24b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(607687, 9)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f670921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/onnx/mapping.py:27: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  int(TensorProto.STRING): np.dtype(np.object)\n"
     ]
    },
    {
     "ename": "MissingShapeCalculator",
     "evalue": "Unable to find a shape calculator for type '<class 'category_encoders.woe.WOEEncoder'>'.\nIt usually means the pipeline being converted contains a\ntransformer or a predictor with no corresponding converter\nimplemented in sklearn-onnx. If the converted is implemented\nin another library, you need to register\nthe converted so that it can be used by sklearn-onnx (function\nupdate_registered_converter). If the model is not yet covered\nby sklearn-onnx, you may raise an issue to\nhttps://github.com/onnx/sklearn-onnx/issues\nto get the converter implemented or even contribute to the\nproject. If the model is a custom model, a new converter must\nbe implemented. Examples can be found in the gallery.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMissingShapeCalculator\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18372/565559214.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m initial_type = [(\"float_input\", FloatTensorType([None, len(context.params[\"features\"]) - len(context.params[\"categorical_features\"])+1])),\n\u001b[1;32m      5\u001b[0m                 (\"categorical_input\", StringTensorType([None, len(context.params[\"categorical_features\"])]))]\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel_onnx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_sklearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/skl2onnx/convert.py\u001b[0m in \u001b[0;36mconvert_sklearn\u001b[0;34m(model, name, initial_types, doc_string, target_opset, custom_conversion_functions, custom_shape_calculators, custom_parsers, options, intermediate, white_op, black_op, final_types, dtype, naming, verbose)\u001b[0m\n\u001b[1;32m    184\u001b[0m     onnx_model = convert_topology(\n\u001b[1;32m    185\u001b[0m         \u001b[0mtopology\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_opset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         remove_identity=not intermediate, verbose=verbose)\n\u001b[0m\u001b[1;32m    187\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[convert_sklearn] end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/skl2onnx/common/_topology.py\u001b[0m in \u001b[0;36mconvert_topology\u001b[0;34m(topology, model_name, doc_string, target_opset, channel_first_inputs, options, remove_identity, verbose)\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;31m# Traverse the graph from roots to leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m     \u001b[0;31m# This loop could eventually be parallelized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m     \u001b[0mtopology\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_operators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m     \u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_topological_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/skl2onnx/common/_topology.py\u001b[0m in \u001b[0;36mconvert_operators\u001b[0;34m(self, container, verbose)\u001b[0m\n\u001b[1;32m   1226\u001b[0m                         \u001b[0m_check_variable_out_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1228\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_shape_calculator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1229\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/skl2onnx/common/_topology.py\u001b[0m in \u001b[0;36mcall_shape_calculator\u001b[0;34m(self, operator)\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[Shape2] call infer_types for %r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m             \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_initialize_graph_status_for_traversing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/skl2onnx/common/_topology.py\u001b[0m in \u001b[0;36minfer_types\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    571\u001b[0m             raise MissingShapeCalculator(\n\u001b[1;32m    572\u001b[0m                 \"Unable to find a shape calculator for type '{}'.\".format(\n\u001b[0;32m--> 573\u001b[0;31m                     type(self.raw_operator)))\n\u001b[0m\u001b[1;32m    574\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mshape_calc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_registration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape_calculator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMissingShapeCalculator\u001b[0m: Unable to find a shape calculator for type '<class 'category_encoders.woe.WOEEncoder'>'.\nIt usually means the pipeline being converted contains a\ntransformer or a predictor with no corresponding converter\nimplemented in sklearn-onnx. If the converted is implemented\nin another library, you need to register\nthe converted so that it can be used by sklearn-onnx (function\nupdate_registered_converter). If the model is not yet covered\nby sklearn-onnx, you may raise an issue to\nhttps://github.com/onnx/sklearn-onnx/issues\nto get the converter implemented or even contribute to the\nproject. If the model is a custom model, a new converter must\nbe implemented. Examples can be found in the gallery.\n"
     ]
    }
   ],
   "source": [
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType, StringTensorType\n",
    "\n",
    "initial_type = [(\"float_input\", FloatTensorType([None, len(context.params[\"features\"]) - len(context.params[\"categorical_features\"])+1])),\n",
    "                (\"categorical_input\", StringTensorType([None, len(context.params[\"categorical_features\"])]))]\n",
    "model_onnx = convert_sklearn(model, initial_types=initial_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36739b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-27 03:08:40,922 - kedro.io.data_catalog - INFO - Loading data from `X_test` (PickleDataSet)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/kedro/framework/context/context.py:488: UserWarning: Credentials not found in your Kedro project config.\n",
      "No files found in ['/kedro-sample/own_examples/conf/base', '/kedro-sample/own_examples/conf/local'] matching the glob pattern(s): ['credentials*', 'credentials*/**', '**/credentials*']\n",
      "  warn(f\"Credentials not found in your Kedro project config.\\n{str(exc)}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(151922, 8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.catalog.load(\"X_test\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85ccf5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-27 03:08:41,689 - kedro.io.data_catalog - INFO - Loading data from `regressor` (PickleDataSet)...\n",
      "2021-12-27 03:08:41,711 - kedro.io.data_catalog - INFO - Saving data to `onx` (PickleDataSet)...\n"
     ]
    }
   ],
   "source": [
    "initial_type = [('float_input', FloatTensorType([None, 8]))]\n",
    "onx = convert_sklearn(context.catalog.load(\"regressor\"), initial_types=initial_type)\n",
    "context.catalog.save(\"onx\", onx.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0474a20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/06_models/shuttles.onnx\", \"wb\") as f:\n",
    "    f.write(onx.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a07d6809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-27 03:08:43,055 - kedro.io.data_catalog - INFO - Loading data from `onx` (PickleDataSet)...\n",
      "2021-12-27 03:08:43,090 - kedro.io.data_catalog - INFO - Loading data from `X_test` (PickleDataSet)...\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as rt\n",
    "import numpy\n",
    "sess = rt.InferenceSession(context.catalog.load(\"onx\"))\n",
    "input_name = sess.get_inputs()[0].name\n",
    "label_name = sess.get_outputs()[0].name\n",
    "pred_onx = sess.run([label_name], {input_name: context.catalog.load(\"X_test\").astype(numpy.float32).values})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2e5df23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5788.9775],\n",
       "       [3973.5654],\n",
       "       [3594.4954],\n",
       "       ...,\n",
       "       [3748.6594],\n",
       "       [5454.851 ],\n",
       "       [4444.2515]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_onx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e6a0c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "faf2c698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OrdinalEncoder as SklOrdinalEncoder\n",
    "from category_encoders import WOEEncoder, OrdinalEncoder\n",
    "from skl2onnx import update_registered_converter, get_model_alias\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "from skl2onnx.common.utils import check_input_and_output_numbers\n",
    "from skl2onnx.algebra.onnx_ops import OnnxCast\n",
    "from skl2onnx.algebra.onnx_operator import OnnxSubEstimator\n",
    "from skl2onnx.sklapi import WOETransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d6966aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordenc_to_sklearn(op_mapping):\n",
    "    \"Converts OrdinalEncoder mapping to scikit-learn OrdinalEncoder.\"\n",
    "    cats = []\n",
    "    for column_map in op_mapping:\n",
    "        col = column_map['col']\n",
    "        while len(cats) <= col:\n",
    "            cats.append(None)\n",
    "        mapping = column_map['mapping']\n",
    "        res = []\n",
    "        for i in range(mapping.shape[0]):\n",
    "            if mapping.index[i]!=mapping.index[i]:\n",
    "                continue\n",
    "            ind = mapping.iloc[i]\n",
    "            while len(res) <= ind:\n",
    "                res.append(0)\n",
    "            res[ind] = mapping.index[i]\n",
    "        cats[col] = np.array(res, dtype=\"O\")\n",
    "\n",
    "    skl_ord = SklOrdinalEncoder(categories=cats, dtype=np.int64)\n",
    "    skl_ord.categories_ = cats\n",
    "    return skl_ord\n",
    "\n",
    "\n",
    "def ordinal_encoder_shape_calculator(operator):\n",
    "    check_input_and_output_numbers(\n",
    "        operator, input_count_range=1, output_count_range=1)\n",
    "    input_type = operator.inputs[0].type.__class__\n",
    "    input_dim = operator.inputs[0].get_first_dimension()\n",
    "    shape = operator.inputs[0].type.shape\n",
    "    second_dim = None if len(shape) != 2 else shape[1]\n",
    "    output_type = input_type([input_dim, second_dim])\n",
    "    operator.outputs[0].type = output_type\n",
    "\n",
    "\n",
    "def ordinal_encoder_converter(scope, operator, container):\n",
    "    op = operator.raw_operator\n",
    "    opv = container.target_opset\n",
    "    X = operator.inputs[0]\n",
    "\n",
    "    skl_ord = ordenc_to_sklearn(op.mapping)\n",
    "    cat = OnnxSubEstimator(skl_ord, X, op_version=opv,\n",
    "                           output_names=operator.outputs[:1])\n",
    "    cat.add_to(scope, container)\n",
    "\n",
    "\n",
    "update_registered_converter(\n",
    "    OrdinalEncoder, \"CategoricalEncoderOrdinalEncoder\",\n",
    "    ordinal_encoder_shape_calculator,\n",
    "    ordinal_encoder_converter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "96df57f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def woeenc_to_sklearn(op_mapping):\n",
    "    \"Converts WOEEncoder mapping to scikit-learn OrdinalEncoder.\"\n",
    "    cats = []\n",
    "    ws = []\n",
    "    for column_map in op_mapping.items():\n",
    "        col = column_map[0]\n",
    "        while len(cats) <= col:\n",
    "            cats.append('passthrough')\n",
    "            ws.append(None)\n",
    "        mapping = column_map[1]\n",
    "        intervals = []\n",
    "        weights = []\n",
    "        for i in range(mapping.shape[0]):\n",
    "            ind = mapping.index[i]\n",
    "            if ind < 0:\n",
    "                continue\n",
    "            intervals.append((float(ind - 1), float(ind), False, True))\n",
    "            weights.append(mapping.iloc[i])\n",
    "        cats[col] = intervals\n",
    "        ws[col] = weights\n",
    "\n",
    "    skl = WOETransformer(intervals=cats, weights=ws, onehot=False)\n",
    "    skl.fit(None)\n",
    "    return skl\n",
    "\n",
    "\n",
    "def woe_encoder_parser(\n",
    "        scope, model, inputs, custom_parsers=None):\n",
    "    if len(inputs) != 1:\n",
    "        raise RuntimeError(\n",
    "            \"Unexpected number of inputs: %d != 1.\" % len(inputs))\n",
    "    if inputs[0].type is None:\n",
    "        raise RuntimeError(\n",
    "            \"Unexpected type: %r.\" % (inputs[0], ))\n",
    "    alias = get_model_alias(type(model))\n",
    "    this_operator = scope.declare_local_operator(alias, model)\n",
    "    this_operator.inputs.append(inputs[0])\n",
    "    this_operator.outputs.append(\n",
    "        scope.declare_local_variable('catwoe', FloatTensorType()))\n",
    "    return this_operator.outputs\n",
    "\n",
    "\n",
    "def woe_encoder_shape_calculator(operator):\n",
    "    check_input_and_output_numbers(\n",
    "        operator, input_count_range=1, output_count_range=1)\n",
    "    input_dim = operator.inputs[0].get_first_dimension()\n",
    "    shape = operator.inputs[0].type.shape\n",
    "    second_dim = None if len(shape) != 2 else shape[1]\n",
    "    output_type = FloatTensorType([input_dim, second_dim])\n",
    "    operator.outputs[0].type = output_type\n",
    "\n",
    "\n",
    "def woe_encoder_converter(scope, operator, container):\n",
    "    op = operator.raw_operator\n",
    "    opv = container.target_opset\n",
    "    X = operator.inputs[0]\n",
    "\n",
    "    sub = OnnxSubEstimator(op.ordinal_encoder, X,\n",
    "                           op_version=opv)\n",
    "    cast = OnnxCast(sub, op_version=opv, to=np.float32)\n",
    "    skl_ord = woeenc_to_sklearn(op.mapping)\n",
    "    cat = OnnxSubEstimator(skl_ord, cast, op_version=opv,\n",
    "                           output_names=operator.outputs[:1],\n",
    "                           input_types=[FloatTensorType()])\n",
    "    cat.add_to(scope, container)\n",
    "\n",
    "\n",
    "update_registered_converter(\n",
    "    WOEEncoder, \"CategoricalEncoderWOEEncoder\",\n",
    "    woe_encoder_shape_calculator,\n",
    "    woe_encoder_converter,\n",
    "    parser=woe_encoder_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0085adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from skl2onnx.common.shape_calculator import calculate_linear_classifier_output_shapes\n",
    "from onnxmltools.convert.xgboost.operator_converters.XGBoost import convert_xgboost\n",
    "\n",
    "update_registered_converter(\n",
    "    XGBClassifier, 'XGBoostXGBClassifier',\n",
    "    calculate_linear_classifier_output_shapes, convert_xgboost,\n",
    "    options={'nocl': [True, False], 'zipmap': [True, False, 'columns']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "df426802",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = XGBClassifier(**context.params[\"model_params\"][\"hyper_parameters\"])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"we\", WOEEncoder()),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"numeric\", \"passthrough\", [context.params[\"features\"].index(cf) for cf in context.params[\"features\"] if cf not in context.params[\"categorical_features\"]]),\n",
    "        (\"categorical\", categorical_transformer, [context.params[\"features\"].index(cf) for cf in context.params[\"categorical_features\"]]),\n",
    "    ],\n",
    "    remainder=\"drop\")\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    (\"precprocessor\", preprocessor),\n",
    "    (\"regressor\", classifier)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "007ec169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-29 02:29:49,518 - kedro.io.data_catalog - INFO - Loading data from `y_train` (PickleDataSet)...\n",
      "2021-12-29 02:29:49,681 - kedro.io.data_catalog - INFO - Loading data from `y_train` (PickleDataSet)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:29:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "ppl = model.fit(\n",
    "    tmp[context.params[\"features\"]].values\n",
    "    ,np.where(context.catalog.load(\"y_train\")<context.catalog.load(\"y_train\").mean(), 0,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "00b5d831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(context.params[\"features\"]) - len(context.params[\"categorical_features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "085ed52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_type = [(\"float_input\", FloatTensorType([None, len(context.params[\"features\"]) - len(context.params[\"categorical_features\"])])),\n",
    "                (\"categorical_input\", StringTensorType([None, len(context.params[\"categorical_features\"])]))]\n",
    "model_onnx = convert_sklearn(model, initial_types=initial_type, target_opset=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f4037d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/06_models/pipeline.onnx\", \"wb\") as f:\n",
    "    f.write(model_onnx.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6aad0bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-29 03:37:02,333 - kedro.io.data_catalog - INFO - Loading data from `X_test` (PickleDataSet)...\n",
      "2021-12-29 03:37:02,418 - kedro.io.data_catalog - INFO - Loading data from `X_test` (PickleDataSet)...\n"
     ]
    }
   ],
   "source": [
    "sess = rt.InferenceSession(model_onnx.SerializeToString())\n",
    "input_name = sess.get_inputs()[0].name\n",
    "label_name = sess.get_outputs()[1].name\n",
    "inpt = {\n",
    "    sess.get_inputs()[0].name: context.catalog.load(\"X_test\")[context.params[\"float_features\"]].astype(numpy.float32).values,\n",
    "    sess.get_inputs()[1].name: context.catalog.load(\"X_test\")[context.params[\"categorical_features\"]].astype(str).values\n",
    "}\n",
    "pred_onx = sess.run([label_name], inpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f21bd330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9922924041748047,\n",
       " 0.09750306606292725,\n",
       " 0.608305037021637,\n",
       " 0.9927235245704651,\n",
       " 0.3346853256225586,\n",
       " 0.01253741979598999,\n",
       " 0.14345026016235352,\n",
       " 0.0792277455329895,\n",
       " 0.02294933795928955,\n",
       " 0.8288884162902832,\n",
       " 0.04639136791229248,\n",
       " 0.5220964550971985,\n",
       " 0.2856330871582031,\n",
       " 0.1634911298751831,\n",
       " 0.9963962435722351,\n",
       " 0.8427826762199402,\n",
       " 0.09037858247756958,\n",
       " 0.4864863157272339,\n",
       " 0.06387436389923096,\n",
       " 0.9724111557006836,\n",
       " 0.20407778024673462,\n",
       " 0.826680064201355,\n",
       " 0.28369295597076416,\n",
       " 0.007855474948883057,\n",
       " 0.9536020159721375,\n",
       " 0.021113038063049316,\n",
       " 0.010698139667510986,\n",
       " 0.9946635961532593,\n",
       " 0.9969168901443481,\n",
       " 0.8948606252670288,\n",
       " 0.07242757081985474,\n",
       " 0.9320152401924133,\n",
       " 0.6635313034057617,\n",
       " 0.3747096657752991,\n",
       " 0.8948606252670288,\n",
       " 0.6432591676712036,\n",
       " 0.014227330684661865,\n",
       " 0.15546762943267822,\n",
       " 0.3346853256225586,\n",
       " 0.29980021715164185,\n",
       " 0.2655649781227112,\n",
       " 0.003201782703399658,\n",
       " 0.004173576831817627,\n",
       " 0.4864863157272339,\n",
       " 0.5220964550971985,\n",
       " 0.0027419328689575195,\n",
       " 0.45635783672332764,\n",
       " 0.03873288631439209,\n",
       " 0.18555617332458496,\n",
       " 0.6432591676712036,\n",
       " 0.7736096382141113,\n",
       " 0.010698139667510986,\n",
       " 0.9051498770713806,\n",
       " 0.09037858247756958,\n",
       " 0.9949551224708557,\n",
       " 0.4864863157272339,\n",
       " 0.040012240409851074,\n",
       " 0.8059289455413818,\n",
       " 0.010695695877075195,\n",
       " 0.08951407670974731,\n",
       " 0.04693633317947388,\n",
       " 0.005128085613250732,\n",
       " 0.3094615936279297,\n",
       " 0.9959779977798462,\n",
       " 0.010268747806549072,\n",
       " 0.9876989126205444,\n",
       " 0.550172746181488,\n",
       " 0.04779845476150513,\n",
       " 0.08951407670974731,\n",
       " 0.7736096382141113,\n",
       " 0.04550284147262573,\n",
       " 0.8288884162902832,\n",
       " 0.08979576826095581,\n",
       " 0.7736096382141113,\n",
       " 0.003201782703399658,\n",
       " 0.362313449382782,\n",
       " 0.5098773837089539,\n",
       " 0.9913395047187805,\n",
       " 0.008129119873046875,\n",
       " 0.45635783672332764,\n",
       " 0.9913617968559265,\n",
       " 0.05169045925140381,\n",
       " 0.006031155586242676,\n",
       " 0.0015695691108703613,\n",
       " 0.45635783672332764,\n",
       " 0.0030208826065063477,\n",
       " 0.0016491413116455078,\n",
       " 0.017726004123687744,\n",
       " 0.2655649781227112,\n",
       " 0.9654042720794678,\n",
       " 0.9956379532814026,\n",
       " 0.5273584127426147,\n",
       " 0.9927235245704651,\n",
       " 0.005971193313598633,\n",
       " 0.026291370391845703,\n",
       " 0.9895628094673157,\n",
       " 0.6165417432785034,\n",
       " 0.03047358989715576,\n",
       " 0.9876989126205444,\n",
       " 0.006838679313659668,\n",
       " 0.010698139667510986,\n",
       " 0.7141778469085693,\n",
       " 0.001536250114440918,\n",
       " 0.14285588264465332,\n",
       " 0.011635184288024902,\n",
       " 0.3346853256225586,\n",
       " 0.2943827509880066,\n",
       " 0.37967556715011597,\n",
       " 0.8158926367759705,\n",
       " 0.007059574127197266,\n",
       " 0.027197957038879395,\n",
       " 0.624478816986084,\n",
       " 0.9933546781539917,\n",
       " 0.3747096657752991,\n",
       " 0.009113430976867676,\n",
       " 0.05362880229949951,\n",
       " 0.6532647609710693,\n",
       " 0.10384052991867065,\n",
       " 0.7231358885765076,\n",
       " 0.9948070645332336,\n",
       " 0.9927235245704651,\n",
       " 0.0313301682472229,\n",
       " 0.004601895809173584,\n",
       " 0.004983007907867432,\n",
       " 0.0009309649467468262,\n",
       " 0.3346853256225586,\n",
       " 0.009113430976867676,\n",
       " 0.9969168901443481,\n",
       " 0.9967954754829407,\n",
       " 0.039655089378356934,\n",
       " 0.022822439670562744,\n",
       " 0.4634970426559448,\n",
       " 0.0006641745567321777,\n",
       " 0.3346853256225586,\n",
       " 0.9967954754829407,\n",
       " 0.9857844710350037,\n",
       " 0.5252236127853394,\n",
       " 0.04149353504180908,\n",
       " 0.9963962435722351,\n",
       " 0.5220964550971985,\n",
       " 0.013500571250915527,\n",
       " 0.9948070645332336,\n",
       " 0.3747096657752991,\n",
       " 0.07210785150527954,\n",
       " 0.007855474948883057,\n",
       " 0.9907249808311462,\n",
       " 0.3346853256225586,\n",
       " 0.7231358885765076,\n",
       " 0.012370586395263672,\n",
       " 0.9178372025489807,\n",
       " 0.007090568542480469,\n",
       " 0.9320152401924133,\n",
       " 0.2655649781227112,\n",
       " 0.0025635361671447754,\n",
       " 0.003315865993499756,\n",
       " 0.0018491148948669434,\n",
       " 0.7736096382141113,\n",
       " 0.8781518340110779,\n",
       " 0.17667019367218018,\n",
       " 0.5252236127853394,\n",
       " 0.6432591676712036,\n",
       " 0.7736096382141113,\n",
       " 0.04779845476150513,\n",
       " 0.03700733184814453,\n",
       " 0.007076859474182129,\n",
       " 0.2342330813407898,\n",
       " 0.991407036781311,\n",
       " 0.4107862710952759,\n",
       " 0.001574873924255371,\n",
       " 0.3747096657752991,\n",
       " 0.3747096657752991,\n",
       " 0.2369709610939026,\n",
       " 0.03858637809753418,\n",
       " 0.5220964550971985,\n",
       " 0.09037858247756958,\n",
       " 0.2340693473815918,\n",
       " 0.0021738409996032715,\n",
       " 0.5220964550971985,\n",
       " 0.04942077398300171,\n",
       " 0.9968492388725281,\n",
       " 0.019267618656158447,\n",
       " 0.00887233018875122,\n",
       " 0.9797749519348145,\n",
       " 0.013628661632537842,\n",
       " 0.0527309775352478,\n",
       " 0.1233367919921875,\n",
       " 0.9863123297691345,\n",
       " 0.12080878019332886,\n",
       " 0.1230173110961914,\n",
       " 0.9956379532814026,\n",
       " 0.000957190990447998,\n",
       " 0.9781476855278015,\n",
       " 0.9917826652526855,\n",
       " 0.550172746181488,\n",
       " 0.0015437602996826172,\n",
       " 0.052600979804992676,\n",
       " 0.9536020159721375,\n",
       " 0.9498997926712036,\n",
       " 0.993739128112793,\n",
       " 0.8059289455413818,\n",
       " 0.7736096382141113,\n",
       " 0.013811290264129639,\n",
       " 0.03917264938354492,\n",
       " 0.9913395047187805,\n",
       " 0.12152928113937378,\n",
       " 0.23876136541366577,\n",
       " 0.9945342540740967,\n",
       " 0.0042822957038879395,\n",
       " 0.031334638595581055,\n",
       " 0.47939735651016235,\n",
       " 0.09037858247756958,\n",
       " 0.4277827739715576,\n",
       " 0.0005449652671813965,\n",
       " 0.0060158371925354,\n",
       " 0.005289196968078613,\n",
       " 0.8059289455413818,\n",
       " 0.15575289726257324,\n",
       " 0.10294777154922485,\n",
       " 0.3369717001914978,\n",
       " 0.022822439670562744,\n",
       " 0.3747096657752991,\n",
       " 0.009342312812805176,\n",
       " 0.9751701951026917,\n",
       " 0.9423566460609436,\n",
       " 0.9624304175376892,\n",
       " 0.0361180305480957,\n",
       " 0.7736096382141113,\n",
       " 0.3747096657752991,\n",
       " 0.998451828956604,\n",
       " 0.25392621755599976,\n",
       " 0.3747096657752991,\n",
       " 0.909360408782959,\n",
       " 0.8943988680839539,\n",
       " 0.14285588264465332,\n",
       " 0.3025689125061035,\n",
       " 0.9433256387710571,\n",
       " 0.3747096657752991,\n",
       " 0.021771788597106934,\n",
       " 0.001438438892364502,\n",
       " 0.21257632970809937,\n",
       " 0.9894808530807495,\n",
       " 0.004298031330108643,\n",
       " 0.0009644031524658203,\n",
       " 0.020936191082000732,\n",
       " 0.4864863157272339,\n",
       " 0.9924001693725586,\n",
       " 0.9962506890296936,\n",
       " 0.1344674825668335,\n",
       " 0.03962022066116333,\n",
       " 0.0361180305480957,\n",
       " 0.026081442832946777,\n",
       " 0.6343278884887695,\n",
       " 0.2853096127510071,\n",
       " 0.004983007907867432,\n",
       " 0.21660608053207397,\n",
       " 0.008957266807556152,\n",
       " 0.0792277455329895,\n",
       " 0.21119290590286255,\n",
       " 0.0010832548141479492,\n",
       " 0.9928621649742126,\n",
       " 0.5014656782150269,\n",
       " 0.5843765735626221,\n",
       " 0.991407036781311,\n",
       " 0.01708090305328369,\n",
       " 0.003264129161834717,\n",
       " 0.5220964550971985,\n",
       " 0.004699349403381348,\n",
       " 0.020154058933258057,\n",
       " 0.004699349403381348,\n",
       " 0.007855474948883057,\n",
       " 0.30977725982666016,\n",
       " 0.8059289455413818,\n",
       " 0.06746035814285278,\n",
       " 0.0010097622871398926,\n",
       " 0.7736096382141113,\n",
       " 0.9921290874481201,\n",
       " 0.1480804681777954,\n",
       " 0.02111649513244629,\n",
       " 0.9907333850860596,\n",
       " 0.007620096206665039,\n",
       " 0.006968498229980469,\n",
       " 0.05466967821121216,\n",
       " 0.013811290264129639,\n",
       " 0.007620096206665039,\n",
       " 0.984553337097168,\n",
       " 0.006741881370544434,\n",
       " 0.8218911290168762,\n",
       " 0.04149353504180908,\n",
       " 0.7231358885765076,\n",
       " 0.0694923996925354,\n",
       " 0.015313327312469482,\n",
       " 0.9924001693725586,\n",
       " 0.007620096206665039,\n",
       " 0.5460849404335022,\n",
       " 0.053353965282440186,\n",
       " 0.12152928113937378,\n",
       " 0.11733853816986084,\n",
       " 0.9536020159721375,\n",
       " 0.022583723068237305,\n",
       " 0.8737159371376038,\n",
       " 0.006121039390563965,\n",
       " 0.2856330871582031,\n",
       " 0.8692733645439148,\n",
       " 0.7696134448051453,\n",
       " 0.9913617968559265,\n",
       " 0.5220964550971985,\n",
       " 0.9806878566741943,\n",
       " 0.010698139667510986,\n",
       " 0.004699349403381348,\n",
       " 0.013811290264129639,\n",
       " 0.12845760583877563,\n",
       " 0.09646499156951904,\n",
       " 0.1230173110961914,\n",
       " 0.016842782497406006,\n",
       " 0.47939735651016235,\n",
       " 0.9948070645332336,\n",
       " 0.14285588264465332,\n",
       " 0.014098525047302246,\n",
       " 0.09037858247756958,\n",
       " 0.007068514823913574,\n",
       " 0.012760937213897705,\n",
       " 0.04149353504180908,\n",
       " 0.022822439670562744,\n",
       " 0.9893990755081177,\n",
       " 0.9962506890296936,\n",
       " 0.007348239421844482,\n",
       " 0.5220964550971985,\n",
       " 0.015383601188659668,\n",
       " 0.9959779977798462,\n",
       " 0.9928178787231445,\n",
       " 0.010698139667510986,\n",
       " 0.9913617968559265,\n",
       " 0.9895628094673157,\n",
       " 0.9778175950050354,\n",
       " 0.3747096657752991,\n",
       " 0.9348530173301697,\n",
       " 0.006741881370544434,\n",
       " 0.09904199838638306,\n",
       " 0.004983007907867432,\n",
       " 0.07136684656143188,\n",
       " 0.9913617968559265,\n",
       " 0.9907333850860596,\n",
       " 0.2790525555610657,\n",
       " 0.9932859539985657,\n",
       " 0.15546762943267822,\n",
       " 0.9957387447357178,\n",
       " 0.7848110795021057,\n",
       " 0.043436646461486816,\n",
       " 0.14158368110656738,\n",
       " 0.9907333850860596,\n",
       " 0.010695695877075195,\n",
       " 0.026081442832946777,\n",
       " 0.004983007907867432,\n",
       " 0.8158926367759705,\n",
       " 0.3747096657752991,\n",
       " 0.4634970426559448,\n",
       " 0.9927235245704651,\n",
       " 0.004699349403381348,\n",
       " 0.20407778024673462,\n",
       " 0.09037858247756958,\n",
       " 0.037805259227752686,\n",
       " 0.2856330871582031,\n",
       " 0.01607060432434082,\n",
       " 0.14285588264465332,\n",
       " 0.02593749761581421,\n",
       " 0.9862680435180664,\n",
       " 0.3747096657752991,\n",
       " 0.020154058933258057,\n",
       " 0.9956379532814026,\n",
       " 0.0935673713684082,\n",
       " 0.052216529846191406,\n",
       " 0.7211288809776306,\n",
       " 0.02020716667175293,\n",
       " 0.030554533004760742,\n",
       " 0.035072267055511475,\n",
       " 0.8059289455413818,\n",
       " 0.043436646461486816,\n",
       " 0.02728372812271118,\n",
       " 0.9778175950050354,\n",
       " 0.0792277455329895,\n",
       " 0.47939735651016235,\n",
       " 0.008615493774414062,\n",
       " 0.012736737728118896,\n",
       " 0.9857844710350037,\n",
       " 0.9261834025382996,\n",
       " 0.9927235245704651,\n",
       " 0.9957387447357178,\n",
       " 0.0040596723556518555,\n",
       " 0.991407036781311,\n",
       " 0.11862540245056152,\n",
       " 0.9423566460609436,\n",
       " 0.04014056921005249,\n",
       " 0.0361180305480957,\n",
       " 0.018388867378234863,\n",
       " 0.9778175950050354,\n",
       " 0.029377400875091553,\n",
       " 0.022822439670562744,\n",
       " 0.45635783672332764,\n",
       " 0.9959779977798462,\n",
       " 0.1557362675666809,\n",
       " 0.7231358885765076,\n",
       " 0.3747096657752991,\n",
       " 0.24498897790908813,\n",
       " 0.05362880229949951,\n",
       " 0.06112337112426758,\n",
       " 0.8059289455413818,\n",
       " 0.39831429719924927,\n",
       " 0.7211288809776306,\n",
       " 0.002945423126220703,\n",
       " 0.1710321307182312,\n",
       " 0.010698139667510986,\n",
       " 0.3828014135360718,\n",
       " 0.021771788597106934,\n",
       " 0.9671779274940491,\n",
       " 0.035072267055511475,\n",
       " 0.028496742248535156,\n",
       " 0.2853096127510071,\n",
       " 0.1475246548652649,\n",
       " 0.10384052991867065,\n",
       " 0.011199593544006348,\n",
       " 0.991407036781311,\n",
       " 0.04149353504180908,\n",
       " 0.9812317490577698,\n",
       " 0.16190892457962036,\n",
       " 0.5460849404335022,\n",
       " 0.007348239421844482,\n",
       " 0.09037858247756958,\n",
       " 0.991407036781311,\n",
       " 0.7848110795021057,\n",
       " 0.9968366622924805,\n",
       " 0.9624304175376892,\n",
       " 0.008402884006500244,\n",
       " 0.052851080894470215,\n",
       " 0.01879328489303589,\n",
       " 0.7240611910820007,\n",
       " 0.013500571250915527,\n",
       " 0.09646499156951904,\n",
       " 0.9638195037841797,\n",
       " 0.0010384321212768555,\n",
       " 0.0003037452697753906,\n",
       " 0.1252814531326294,\n",
       " 0.006138801574707031,\n",
       " 0.0025635361671447754,\n",
       " 0.9200274348258972,\n",
       " 0.15575289726257324,\n",
       " 0.010698139667510986,\n",
       " 0.09037858247756958,\n",
       " 0.006829380989074707,\n",
       " 0.010102987289428711,\n",
       " 0.011004984378814697,\n",
       " 0.9949723482131958,\n",
       " 0.00796198844909668,\n",
       " 0.19515007734298706,\n",
       " 0.013159394264221191,\n",
       " 0.9465274214744568,\n",
       " 0.9948070645332336,\n",
       " 0.17667019367218018,\n",
       " 0.19655323028564453,\n",
       " 0.37553054094314575,\n",
       " 0.3747096657752991,\n",
       " 0.10478460788726807,\n",
       " 0.007068514823913574,\n",
       " 0.006972610950469971,\n",
       " 0.3346853256225586,\n",
       " 0.022822439670562744,\n",
       " 0.9773771166801453,\n",
       " 0.9923723936080933,\n",
       " 0.21981799602508545,\n",
       " 0.0022644996643066406,\n",
       " 0.991407036781311,\n",
       " 0.06812793016433716,\n",
       " 0.9536020159721375,\n",
       " 0.005568563938140869,\n",
       " 0.8957037925720215,\n",
       " 0.04385095834732056,\n",
       " 0.6620468497276306,\n",
       " 0.004179120063781738,\n",
       " 0.9905560612678528,\n",
       " 0.2741866111755371,\n",
       " 0.9893990755081177,\n",
       " 0.006346464157104492,\n",
       " 0.9169431924819946,\n",
       " 0.9969168901443481,\n",
       " 0.011380910873413086,\n",
       " 0.3747096657752991,\n",
       " 0.26743632555007935,\n",
       " 0.9200274348258972,\n",
       " 0.20407778024673462,\n",
       " 0.14285588264465332,\n",
       " 0.3747096657752991,\n",
       " 0.7848110795021057,\n",
       " 0.9893990755081177,\n",
       " 0.008413434028625488,\n",
       " 0.49564266204833984,\n",
       " 0.011159956455230713,\n",
       " 0.006958961486816406,\n",
       " 0.25392621755599976,\n",
       " 0.006089448928833008,\n",
       " 0.02588498592376709,\n",
       " 0.05466967821121216,\n",
       " 0.07055264711380005,\n",
       " 0.0037587881088256836,\n",
       " 0.5930107831954956,\n",
       " 0.021883487701416016,\n",
       " 0.025652945041656494,\n",
       " 0.3747096657752991,\n",
       " 0.991407036781311,\n",
       " 0.013074636459350586,\n",
       " 0.9051035046577454,\n",
       " 0.9961481094360352,\n",
       " 0.9965246319770813,\n",
       " 0.5220964550971985,\n",
       " 0.9323527812957764,\n",
       " 0.0021800994873046875,\n",
       " 0.9927235245704651,\n",
       " 0.01902759075164795,\n",
       " 0.0031180381774902344,\n",
       " 0.007620096206665039,\n",
       " 0.1480804681777954,\n",
       " 0.9907333850860596,\n",
       " 0.004983007907867432,\n",
       " 0.22193259000778198,\n",
       " 0.3747096657752991,\n",
       " 0.3346853256225586,\n",
       " 0.9894808530807495,\n",
       " 0.37553054094314575,\n",
       " 0.057359278202056885,\n",
       " 0.3346853256225586,\n",
       " 0.09037858247756958,\n",
       " 0.9949723482131958,\n",
       " 0.02185988426208496,\n",
       " 0.9956379532814026,\n",
       " 0.006760835647583008,\n",
       " 0.6432591676712036,\n",
       " 0.0042822957038879395,\n",
       " 0.03917264938354492,\n",
       " 0.003838062286376953,\n",
       " 0.09037858247756958,\n",
       " 0.0792277455329895,\n",
       " 0.9178372025489807,\n",
       " 0.9812317490577698,\n",
       " 0.5252236127853394,\n",
       " 0.6590872406959534,\n",
       " 0.0027419328689575195,\n",
       " 0.09048658609390259,\n",
       " 0.14345026016235352,\n",
       " 0.020154058933258057,\n",
       " 0.008859813213348389,\n",
       " 0.991407036781311,\n",
       " 0.00668710470199585,\n",
       " 0.9778175950050354,\n",
       " 0.012370586395263672,\n",
       " 0.25392621755599976,\n",
       " 0.3230683207511902,\n",
       " 0.991407036781311,\n",
       " 0.2602853775024414,\n",
       " 0.590724766254425,\n",
       " 0.33069121837615967,\n",
       " 0.021394431591033936,\n",
       " 0.9968366622924805,\n",
       " 0.01831871271133423,\n",
       " 0.0003037452697753906,\n",
       " 0.009489119052886963,\n",
       " 0.07837879657745361,\n",
       " 0.9923723936080933,\n",
       " 0.23703521490097046,\n",
       " 0.7934244871139526,\n",
       " 0.664616048336029,\n",
       " 0.019152522087097168,\n",
       " 0.3346853256225586,\n",
       " 0.005289196968078613,\n",
       " 0.9178372025489807,\n",
       " 0.9857063889503479,\n",
       " 0.043436646461486816,\n",
       " 0.03917264938354492,\n",
       " 0.7736096382141113,\n",
       " 0.9905560612678528,\n",
       " 0.0012912750244140625,\n",
       " 0.9155704975128174,\n",
       " 0.9751701951026917,\n",
       " 0.012076795101165771,\n",
       " 0.664616048336029,\n",
       " 0.8577418327331543,\n",
       " 0.02702254056930542,\n",
       " 0.5252236127853394,\n",
       " 0.8442478775978088,\n",
       " 0.21981799602508545,\n",
       " 0.035072267055511475,\n",
       " 0.3747096657752991,\n",
       " 0.991407036781311,\n",
       " 0.07242757081985474,\n",
       " 0.3828014135360718,\n",
       " 0.006003141403198242,\n",
       " 0.011714696884155273,\n",
       " 0.7288476228713989,\n",
       " 0.8218911290168762,\n",
       " 0.7231358885765076,\n",
       " 0.0065863728523254395,\n",
       " 0.612166702747345,\n",
       " 0.9967954754829407,\n",
       " 0.34318363666534424,\n",
       " 0.05291086435317993,\n",
       " 0.7848110795021057,\n",
       " 0.17975658178329468,\n",
       " 0.004007399082183838,\n",
       " 0.3747096657752991,\n",
       " 0.012370586395263672,\n",
       " 0.11049222946166992,\n",
       " 0.012276649475097656,\n",
       " 0.7848110795021057,\n",
       " 0.09037858247756958,\n",
       " 0.09037858247756958,\n",
       " 0.011494696140289307,\n",
       " 0.7728002667427063,\n",
       " 0.9787350296974182,\n",
       " 0.3747096657752991,\n",
       " 0.608305037021637,\n",
       " 0.590724766254425,\n",
       " 0.07242757081985474,\n",
       " 0.12152928113937378,\n",
       " 0.9797749519348145,\n",
       " 0.007855474948883057,\n",
       " 0.9778175950050354,\n",
       " 0.9549745321273804,\n",
       " 0.550172746181488,\n",
       " 0.2655649781227112,\n",
       " 0.006303608417510986,\n",
       " 0.9815571308135986,\n",
       " 0.9907333850860596,\n",
       " 0.7231358885765076,\n",
       " 0.8218911290168762,\n",
       " 0.010695695877075195,\n",
       " 0.9949723482131958,\n",
       " 0.0006903409957885742,\n",
       " 0.22193259000778198,\n",
       " 0.007855474948883057,\n",
       " 0.09037858247756958,\n",
       " 0.11862540245056152,\n",
       " 0.007855474948883057,\n",
       " 0.3747096657752991,\n",
       " 0.022401809692382812,\n",
       " 0.98894202709198,\n",
       " 0.00408780574798584,\n",
       " 0.9601998925209045,\n",
       " 0.003624856472015381,\n",
       " 0.6590872406959534,\n",
       " 0.4634970426559448,\n",
       " 0.14285588264465332,\n",
       " 0.7211288809776306,\n",
       " 0.22193259000778198,\n",
       " 0.9927235245704651,\n",
       " 0.04614502191543579,\n",
       " 0.012076795101165771,\n",
       " 0.33434420824050903,\n",
       " 0.02059483528137207,\n",
       " 0.5220964550971985,\n",
       " 0.17975658178329468,\n",
       " 0.3792598247528076,\n",
       " 0.015313327312469482,\n",
       " 0.021844685077667236,\n",
       " 0.9895628094673157,\n",
       " 0.9936519861221313,\n",
       " 0.009068727493286133,\n",
       " 0.6590872406959534,\n",
       " 0.9949723482131958,\n",
       " 0.02053844928741455,\n",
       " 0.009182572364807129,\n",
       " 0.3747096657752991,\n",
       " 0.0004093050956726074,\n",
       " 0.006760835647583008,\n",
       " 0.02156996726989746,\n",
       " 0.0361180305480957,\n",
       " 0.9536020159721375,\n",
       " 0.07242757081985474,\n",
       " 0.9778175950050354,\n",
       " 0.0015695691108703613,\n",
       " 0.9927235245704651,\n",
       " 0.004601895809173584,\n",
       " 0.991407036781311,\n",
       " 0.8059289455413818,\n",
       " 0.22193259000778198,\n",
       " 0.013953030109405518,\n",
       " 0.98894202709198,\n",
       " 0.015592455863952637,\n",
       " 0.608305037021637,\n",
       " 0.027738511562347412,\n",
       " 0.4116140604019165,\n",
       " 0.9863123297691345,\n",
       " 0.02167665958404541,\n",
       " 0.98894202709198,\n",
       " 0.9812317490577698,\n",
       " 0.01253741979598999,\n",
       " 0.49564266204833984,\n",
       " 0.9913617968559265,\n",
       " 0.022822439670562744,\n",
       " 0.006138801574707031,\n",
       " 0.3747096657752991,\n",
       " 0.010698139667510986,\n",
       " 0.0029351115226745605,\n",
       " 0.9423566460609436,\n",
       " 0.0008503198623657227,\n",
       " 0.21262049674987793,\n",
       " 0.2702704071998596,\n",
       " 0.9969168901443481,\n",
       " 0.47939735651016235,\n",
       " 0.6590872406959534,\n",
       " 0.9903936982154846,\n",
       " 0.9907333850860596,\n",
       " 0.09750306606292725,\n",
       " 0.2856330871582031,\n",
       " 0.005944252014160156,\n",
       " 0.057359278202056885,\n",
       " 0.9894808530807495,\n",
       " 0.3747096657752991,\n",
       " 0.9917826652526855,\n",
       " 0.9907333850860596,\n",
       " 0.05169045925140381,\n",
       " 0.4864863157272339,\n",
       " 0.13266581296920776,\n",
       " 0.9948070645332336,\n",
       " 0.12593138217926025,\n",
       " 0.9894053339958191,\n",
       " 0.5252236127853394,\n",
       " 0.01724797487258911,\n",
       " 0.41076886653900146,\n",
       " 0.9900408387184143,\n",
       " 0.012165665626525879,\n",
       " 0.9178372025489807,\n",
       " 0.008391857147216797,\n",
       " 0.2853096127510071,\n",
       " 0.012076795101165771,\n",
       " 0.9346225261688232,\n",
       " 0.9967954754829407,\n",
       " 0.014562606811523438,\n",
       " 0.06387436389923096,\n",
       " 0.8158926367759705,\n",
       " 0.9968492388725281,\n",
       " 0.6590872406959534,\n",
       " 0.004298031330108643,\n",
       " 0.007855474948883057,\n",
       " 0.09037858247756958,\n",
       " 0.19835418462753296,\n",
       " 0.0054972171783447266,\n",
       " 0.007855474948883057,\n",
       " 0.02728372812271118,\n",
       " 0.005289196968078613,\n",
       " 0.9968492388725281,\n",
       " 0.3747096657752991,\n",
       " 0.9936519861221313,\n",
       " 0.9927235245704651,\n",
       " 0.9051035046577454,\n",
       " 0.2563944458961487,\n",
       " 0.0361180305480957,\n",
       " 0.21981799602508545,\n",
       " 0.8059289455413818,\n",
       " 0.7934244871139526,\n",
       " 0.02129077911376953,\n",
       " 0.09037858247756958,\n",
       " 0.0036692023277282715,\n",
       " 0.011519551277160645,\n",
       " 0.009113430976867676,\n",
       " 0.9907249808311462,\n",
       " 0.7231358885765076,\n",
       " 0.993739128112793,\n",
       " 0.550172746181488,\n",
       " 0.9724111557006836,\n",
       " 0.6159657835960388,\n",
       " 0.9927235245704651,\n",
       " 0.9820137619972229,\n",
       " 0.008129119873046875,\n",
       " 0.9948070645332336,\n",
       " 0.006760835647583008,\n",
       " 0.012291312217712402,\n",
       " 0.9348530173301697,\n",
       " 0.007855474948883057,\n",
       " 0.04619568586349487,\n",
       " 0.0024144649505615234,\n",
       " 0.011439979076385498,\n",
       " 0.0013478398323059082,\n",
       " 0.6720420122146606,\n",
       " 0.9917826652526855,\n",
       " 0.9913617968559265,\n",
       " 0.006799519062042236,\n",
       " 0.4864863157272339,\n",
       " 0.02294933795928955,\n",
       " 0.9957387447357178,\n",
       " 0.3230683207511902,\n",
       " 0.022822439670562744,\n",
       " 0.03917264938354492,\n",
       " 0.9967954754829407,\n",
       " 0.14285588264465332,\n",
       " 0.0021800994873046875,\n",
       " 0.9239447712898254,\n",
       " 0.004179120063781738,\n",
       " 0.04279607534408569,\n",
       " 0.08794933557510376,\n",
       " 0.37800371646881104,\n",
       " 0.3346853256225586,\n",
       " 0.0792277455329895,\n",
       " 0.09037858247756958,\n",
       " 0.9051035046577454,\n",
       " 0.5220964550971985,\n",
       " 0.5252236127853394,\n",
       " 0.5252236127853394,\n",
       " 0.01927083730697632,\n",
       " 0.04619568586349487,\n",
       " 0.007517874240875244,\n",
       " 0.9926449060440063,\n",
       " 0.28203368186950684,\n",
       " 0.3346853256225586,\n",
       " 0.003315865993499756,\n",
       " 0.9932859539985657,\n",
       " 0.013953030109405518,\n",
       " 0.018998801708221436,\n",
       " 0.6620468497276306,\n",
       " 0.9967954754829407,\n",
       " 0.624478816986084,\n",
       " 0.018132686614990234,\n",
       " 0.0012912750244140625,\n",
       " 0.0008637309074401855,\n",
       " 0.067344069480896,\n",
       " 0.3346853256225586,\n",
       " 0.05304771661758423,\n",
       " 0.9346901774406433,\n",
       " 0.022583723068237305,\n",
       " 0.22024887800216675,\n",
       " 0.9922575950622559,\n",
       " 0.04070913791656494,\n",
       " 0.5252236127853394,\n",
       " 0.03917264938354492,\n",
       " 0.5252236127853394,\n",
       " 0.007620096206665039,\n",
       " 0.2655649781227112,\n",
       " 0.3747096657752991,\n",
       " 0.9959779977798462,\n",
       " 0.7785035967826843,\n",
       " 0.9968366622924805,\n",
       " 0.2088913917541504,\n",
       " 0.0004981756210327148,\n",
       " 0.010015726089477539,\n",
       " 0.7848110795021057,\n",
       " 0.9959779977798462,\n",
       " 0.991407036781311,\n",
       " 0.7785035967826843,\n",
       " 0.9903936982154846,\n",
       " 0.9465274214744568,\n",
       " 0.9926449060440063,\n",
       " 0.0005200505256652832,\n",
       " 0.0021738409996032715,\n",
       " 0.13295841217041016,\n",
       " 0.9862680435180664,\n",
       " 0.6590872406959534,\n",
       " 0.3747096657752991,\n",
       " 0.007855474948883057,\n",
       " 0.013362646102905273,\n",
       " 0.013811290264129639,\n",
       " 0.0018491148948669434,\n",
       " 0.02053844928741455,\n",
       " 0.9178372025489807,\n",
       " 0.5252236127853394,\n",
       " 0.5220964550971985,\n",
       " 0.7736096382141113,\n",
       " 0.0007318854331970215,\n",
       " 0.550172746181488,\n",
       " 0.004822492599487305,\n",
       " 0.0006903409957885742,\n",
       " 0.022822439670562744,\n",
       " 0.0941929817199707,\n",
       " 0.3806707262992859,\n",
       " 0.009113430976867676,\n",
       " 0.6839318871498108,\n",
       " 0.006138801574707031,\n",
       " 0.3346853256225586,\n",
       " 0.5252236127853394,\n",
       " 0.6165417432785034,\n",
       " 0.04029858112335205,\n",
       " 0.021771788597106934,\n",
       " 0.9051035046577454,\n",
       " 0.020936191082000732,\n",
       " 0.009342312812805176,\n",
       " 0.7696134448051453,\n",
       " 0.010866105556488037,\n",
       " 0.5200397372245789,\n",
       " 0.055590689182281494,\n",
       " 0.035072267055511475,\n",
       " 0.8059289455413818,\n",
       " 0.9200274348258972,\n",
       " 0.9239447712898254,\n",
       " 0.0014516711235046387,\n",
       " 0.3230683207511902,\n",
       " 0.0040885210037231445,\n",
       " 0.5248275995254517,\n",
       " 0.3747096657752991,\n",
       " 0.414154589176178,\n",
       " 0.5433605909347534,\n",
       " 0.6165417432785034,\n",
       " 0.022583723068237305,\n",
       " 0.5200397372245789,\n",
       " 0.016498923301696777,\n",
       " 0.052600979804992676,\n",
       " 0.98894202709198,\n",
       " 0.9913617968559265,\n",
       " 0.05183601379394531,\n",
       " 0.5252236127853394,\n",
       " 0.7231358885765076,\n",
       " 0.0013971924781799316,\n",
       " 0.9836733341217041,\n",
       " 0.7211288809776306,\n",
       " 0.9968366622924805,\n",
       " 0.7231358885765076,\n",
       " 0.0034855008125305176,\n",
       " 0.3346853256225586,\n",
       " 0.01542586088180542,\n",
       " 0.21981799602508545,\n",
       " 0.007068514823913574,\n",
       " 0.37553054094314575,\n",
       " 0.9961481094360352,\n",
       " 0.09655696153640747,\n",
       " 0.016498923301696777,\n",
       " 0.007855474948883057,\n",
       " 0.9372338056564331,\n",
       " 0.9893990755081177,\n",
       " 0.7579280138015747,\n",
       " 0.09116244316101074,\n",
       " 0.998451828956604,\n",
       " 0.4634970426559448,\n",
       " 0.993739128112793,\n",
       " 0.9787350296974182,\n",
       " 0.004822492599487305,\n",
       " 0.0010097622871398926,\n",
       " 0.9923723936080933,\n",
       " 0.550172746181488,\n",
       " 0.8059289455413818,\n",
       " 0.9962506890296936,\n",
       " 0.08586353063583374,\n",
       " 0.0054839253425598145,\n",
       " 0.41842883825302124,\n",
       " 0.9784963130950928,\n",
       " 0.020883381366729736,\n",
       " 0.15575289726257324,\n",
       " 0.0026616454124450684,\n",
       " 0.11862540245056152,\n",
       " 0.21981799602508545,\n",
       " 0.0017014145851135254,\n",
       " 0.7438766956329346,\n",
       " 0.015605330467224121,\n",
       " 0.2856330871582031,\n",
       " 0.009113430976867676,\n",
       " 0.8059289455413818,\n",
       " 0.6590872406959534,\n",
       " 0.1578626036643982,\n",
       " 0.6432591676712036,\n",
       " 0.010698139667510986,\n",
       " 0.021883487701416016,\n",
       " 0.04046887159347534,\n",
       " 0.13109958171844482,\n",
       " 0.98894202709198,\n",
       " 0.7934244871139526,\n",
       " 0.004173576831817627,\n",
       " 0.0025832653045654297,\n",
       " 0.7785035967826843,\n",
       " 0.08793216943740845,\n",
       " 0.004557251930236816,\n",
       " 0.9927235245704651,\n",
       " 0.020154058933258057,\n",
       " 0.054429709911346436,\n",
       " 0.0024635791778564453,\n",
       " 0.19749963283538818,\n",
       " 0.15525293350219727,\n",
       " 0.5220964550971985,\n",
       " 0.046779751777648926,\n",
       " 0.9912669062614441,\n",
       " 0.4110291600227356,\n",
       " 0.19707798957824707,\n",
       " 0.5220964550971985,\n",
       " 0.022822439670562744,\n",
       " 0.25684285163879395,\n",
       " 0.9962506890296936,\n",
       " 0.008391857147216797,\n",
       " 0.015836775302886963,\n",
       " 0.010698139667510986,\n",
       " 0.14158368110656738,\n",
       " 0.991407036781311,\n",
       " 0.016498923301696777,\n",
       " 0.9240830540657043,\n",
       " 0.8948606252670288,\n",
       " 0.1344674825668335,\n",
       " 0.04149353504180908,\n",
       " 0.9372338056564331,\n",
       " 0.9926449060440063,\n",
       " 0.6432591676712036,\n",
       " 0.32402902841567993,\n",
       " 0.6432591676712036,\n",
       " 0.1290956735610962,\n",
       " 0.9926449060440063,\n",
       " 0.9423566460609436,\n",
       " 0.4634970426559448,\n",
       " 0.6432591676712036,\n",
       " 0.991407036781311,\n",
       " 0.7848110795021057,\n",
       " ...]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v[1] for v in pred_onx[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895011b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "HellowKedro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
